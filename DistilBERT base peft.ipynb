{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7q8pcE63OXFfdC61ALPls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aGgnzgzJUtuv"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","import numpy as np\n","from torch.utils.data import Dataset\n","from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, matthews_corrcoef, cohen_kappa_score\n","from transformers import EvalPrediction, TrainingArguments, Trainer\n","from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n","\n","# Load and preprocess data\n","df = pd.read_csv('drive/MyDrive/preprocessed.csv')\n","rng_seed = 100\n","df_randomized_order = df.sample(frac=1, random_state=rng_seed)\n","df_randomized_order = df_randomized_order[df_randomized_order[\"section_code\"] != \"-\"]\n","df_randomized_order['section_code'] = df_randomized_order['section_code'].str.split(',')\n","\n","# Multi-label binarization\n","multilabel = MultiLabelBinarizer()\n","labels = multilabel.fit_transform(df_randomized_order['section_code']).astype('float32')\n","texts = df_randomized_order['abstracted_heading_plus_content'].tolist()\n","\n","# Balancing the dataset\n","def balance_dataset(texts, labels):\n","    total_instances = len(texts)\n","    balanced_texts = []\n","    balanced_labels = []\n","\n","    for i in range(labels.shape[1]):\n","        class_indices = np.where(labels[:, i] == 1)[0]\n","        class_texts = [texts[j] for j in class_indices]\n","        class_labels = labels[class_indices]\n","\n","        num_dup = total_instances // len(class_texts)\n","        balanced_class_texts = class_texts * num_dup\n","        balanced_class_labels = np.tile(class_labels, (num_dup, 1))\n","\n","        num_add = total_instances % len(class_texts)\n","        resampled_texts = resample(class_texts, n_samples=num_add, random_state=0)\n","        resampled_labels = resample(class_labels, n_samples=num_add, random_state=0)\n","\n","        balanced_class_texts.extend(resampled_texts)\n","        balanced_class_labels = np.vstack((balanced_class_labels, resampled_labels))\n","\n","        balanced_texts.extend(balanced_class_texts)\n","        balanced_labels.append(balanced_class_labels)\n","\n","    balanced_labels = np.vstack(balanced_labels)\n","    return balanced_texts, balanced_labels\n","\n","balanced_texts, balanced_labels = balance_dataset(texts, labels)\n","\n","# Train-test split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(balanced_texts, balanced_labels, test_size=0.3, random_state=42)\n","\n","# Load tokenizer and model\n","checkpoint = 'distilbert-base-uncased'\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=labels.shape[1])\n","\n","# Custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = torch.tensor(self.labels[idx])\n","        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': label\n","        }\n","\n","train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n","val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n","\n","def multi_labels_metrics(predictions, labels, threshold=0.3):\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(predictions))\n","    y_pred = np.zeros(probs.shape)\n","    y_pred[np.where(probs >= threshold)] = 1\n","    y_true = labels\n","\n","    f1_macro = f1_score(y_true, y_pred, average='macro')\n","    f1_micro = f1_score(y_true, y_pred, average='micro')\n","    roc_auc_macro = roc_auc_score(y_true, y_pred, average='macro')\n","    roc_auc_micro = roc_auc_score(y_true, y_pred, average='micro')\n","    hamming = hamming_loss(y_true, y_pred)\n","\n","    # Label-wise weighted MCC calculation\n","    num_labels = y_true.shape[1]\n","    mcc_weighted = 0\n","    total_weight = 0\n","\n","    for i in range(num_labels):\n","        y_true_label = y_true[:, i]\n","        y_pred_label = y_pred[:, i]\n","\n","        # Weight by the proportion of positive instances for the label\n","        weight = float(y_true_label.sum()) / y_true_label.shape[0]\n","        total_weight += weight\n","        mcc_weighted += weight * matthews_corrcoef(y_true_label, y_pred_label)\n","\n","    # Normalize by total weight\n","    if total_weight > 0:\n","        mcc_weighted /= total_weight\n","\n","    # Weighted Kappa calculation (if needed)\n","    y_true_flat = y_true.flatten()\n","    y_pred_flat = y_pred.flatten()\n","    kappa_weighted = cohen_kappa_score(y_true_flat, y_pred_flat, weights=\"quadratic\")\n","\n","    metrics = {\n","        \"roc_auc_macro\": roc_auc_macro,\n","        \"roc_auc_micro\": roc_auc_micro,\n","        \"hamming_loss\": hamming,\n","        \"f1_macro\": f1_macro,\n","        \"f1_micro\": f1_micro,\n","        \"mcc_weighted\": mcc_weighted,\n","        \"kappa_weighted\": kappa_weighted\n","    }\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    result = multi_labels_metrics(predictions=preds, labels=p.label_ids)\n","    return result\n","\n","peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n","                         r=4,\n","                         lora_alpha=32,\n","                         lora_dropout=0.01,\n","                         target_modules=['query'])\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","# Training Arguments without saving the model\n","args = TrainingArguments(\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    output_dir='./drive/MyDrive/fit',\n","    num_train_epochs=6,\n","    evaluation_strategy=\"epoch\",\n","    logging_dir='./drive/MyDrive/logs',\n","    save_strategy=\"epoch\"\n",")\n","\n","# Trainer initialization\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n",")\n"]}]}